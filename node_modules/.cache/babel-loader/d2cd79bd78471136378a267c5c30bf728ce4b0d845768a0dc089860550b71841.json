{"ast":null,"code":"var _jsxFileName = \"/Users/wesleylu/Desktop/wlu314/src/Pages/VIP/VIP.jsx\";\nimport React from 'react';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nfunction VIP() {\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"h1\", {\n      children: \"VIP Program\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 6,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n      children: [/*#__PURE__*/_jsxDEV(\"strong\", {\n        children: \"Neural Architecture Search (NAS)\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 9\n      }, this), \" is a technique for automating the design of artificial neural networks used in machine learning. NAS has designed networks that are on par with or outperform hand-designed architectures. The methods for NAS are categorized into \", /*#__PURE__*/_jsxDEV(\"strong\", {\n        children: \"search space, search strategy, and performance estimation\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 287\n      }, this), \".\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 7,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n      children: [\"An approach to NAS is based on \", /*#__PURE__*/_jsxDEV(\"strong\", {\n        children: \"evolutionary algorithms\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 11,\n        columnNumber: 40\n      }, this), \". An evolutionary algorithm for NAS performs the following steps:\", /*#__PURE__*/_jsxDEV(\"ul\", {\n        children: [/*#__PURE__*/_jsxDEV(\"li\", {\n          children: \"The pool consists of candidate architectures along with their validation scores.\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 13,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n          children: \"At each step, the architectures are mutated (e.g., replacing a 5x5 convolution with a 3x3 convolution).\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 14,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n          children: \"The new architectures are trained from scratch.\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 15,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n          children: \"The lowest-scoring architectures in the candidate pool are replaced with better ones.\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 16,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 12,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 10,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n      children: [/*#__PURE__*/_jsxDEV(\"strong\", {\n        children: \"Bayesian Optimization (BO)\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 20,\n        columnNumber: 9\n      }, this), \" is a method for hyperparameter optimization that can also be applied to NAS. The objective function maps an architecture to its validation error after being trained for a number of epochs. BO uses a surrogate model to estimate this objective function based on previously evaluated architectures.\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 19,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n      children: [\"RL or evolution-based NAS traditionally required thousands of GPU days of searching and training to achieve state-of-the-art results. To reduce computational costs, many recent NAS methods rely on the \", /*#__PURE__*/_jsxDEV(\"strong\", {\n        children: \"weight-sharing idea\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 23,\n        columnNumber: 210\n      }, this), \". A supernetwork is a very large\", ' ', /*#__PURE__*/_jsxDEV(\"a\", {\n        href: \"https://en.wikipedia.org/wiki/Directed_acyclic_graph\",\n        target: \"_blank\",\n        rel: \"noopener noreferrer\",\n        children: \"Directed Acyclic Graph (DAG)\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 24,\n        columnNumber: 9\n      }, this), ' ', \"whose subgraphs are different candidate neural networks. In a supernetwork, the weights are shared among a large number of sub-architectures that have edges in common, with each considered a path within the supernet. The essential idea is to train one supernetwork that spans many options for the final design rather than generating and training thousands of networks independently.\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 22,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n      children: [\"NAS has traditionally required substantial computational resources due to its intensive training and evaluation phases, leading to a significant carbon footprint. To address this, \", /*#__PURE__*/_jsxDEV(\"strong\", {\n        children: \"NAS benchmarks\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 30,\n        columnNumber: 189\n      }, this), \" have been developed, allowing rapid querying or prediction of final performance for neural architectures, drastically reducing computational requirements.\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 29,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n      children: [\"A \", /*#__PURE__*/_jsxDEV(\"strong\", {\n        children: \"NAS benchmark\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 33,\n        columnNumber: 11\n      }, this), \" is a dataset structured with a predetermined train-test split, a defined search space, and a consistent training pipeline (with fixed hyperparameters). These benchmarks fall into two main categories:\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 32,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"ol\", {\n      children: [/*#__PURE__*/_jsxDEV(\"li\", {\n        children: [/*#__PURE__*/_jsxDEV(\"strong\", {\n          children: \"Surrogate NAS Benchmarks:\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 37,\n          columnNumber: 11\n        }, this), \" These use a \", /*#__PURE__*/_jsxDEV(\"strong\", {\n          children: \"surrogate model\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 37,\n          columnNumber: 66\n        }, this), \" (often a neural network) to predict the performance of an architecture within the search space, enabling faster evaluation by bypassing full training.\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 36,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"li\", {\n        children: [/*#__PURE__*/_jsxDEV(\"strong\", {\n          children: \"Tabular NAS Benchmarks:\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 40,\n          columnNumber: 11\n        }, this), \" These provide a precomputed \", /*#__PURE__*/_jsxDEV(\"strong\", {\n          children: \"lookup table\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 40,\n          columnNumber: 80\n        }, this), \" with actual performance metrics for architectures trained to convergence. Querying these tables yields precise results without needing to perform new, costly training runs.\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 39,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n      children: [\"Both types of benchmarks are highly \", /*#__PURE__*/_jsxDEV(\"strong\", {\n        children: \"queryable\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 44,\n        columnNumber: 45\n      }, this), \" and can simulate many NAS algorithms effectively, often using only a CPU for queries. This approach streamlines the NAS process, saving both time and energy by eliminating the need to train each candidate architecture from scratch.\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 43,\n      columnNumber: 7\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 5,\n    columnNumber: 5\n  }, this);\n}\n_c = VIP;\nexport default VIP;\nvar _c;\n$RefreshReg$(_c, \"VIP\");","map":{"version":3,"names":["React","jsxDEV","_jsxDEV","VIP","children","fileName","_jsxFileName","lineNumber","columnNumber","href","target","rel","_c","$RefreshReg$"],"sources":["/Users/wesleylu/Desktop/wlu314/src/Pages/VIP/VIP.jsx"],"sourcesContent":["import React from 'react';\n\nfunction VIP() {\n  return (\n    <div>\n      <h1>VIP Program</h1>\n      <p>\n        <strong>Neural Architecture Search (NAS)</strong> is a technique for automating the design of artificial neural networks used in machine learning. NAS has designed networks that are on par with or outperform hand-designed architectures. The methods for NAS are categorized into <strong>search space, search strategy, and performance estimation</strong>.\n      </p>\n      <p>\n        An approach to NAS is based on <strong>evolutionary algorithms</strong>. An evolutionary algorithm for NAS performs the following steps:\n        <ul>\n          <li>The pool consists of candidate architectures along with their validation scores.</li>\n          <li>At each step, the architectures are mutated (e.g., replacing a 5x5 convolution with a 3x3 convolution).</li>\n          <li>The new architectures are trained from scratch.</li>\n          <li>The lowest-scoring architectures in the candidate pool are replaced with better ones.</li>\n        </ul>\n      </p>\n      <p>\n        <strong>Bayesian Optimization (BO)</strong> is a method for hyperparameter optimization that can also be applied to NAS. The objective function maps an architecture to its validation error after being trained for a number of epochs. BO uses a surrogate model to estimate this objective function based on previously evaluated architectures.\n      </p>\n      <p>\n        RL or evolution-based NAS traditionally required thousands of GPU days of searching and training to achieve state-of-the-art results. To reduce computational costs, many recent NAS methods rely on the <strong>weight-sharing idea</strong>. A supernetwork is a very large{' '}\n        <a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\" target=\"_blank\" rel=\"noopener noreferrer\">\n          Directed Acyclic Graph (DAG)\n        </a>{' '}\n        whose subgraphs are different candidate neural networks. In a supernetwork, the weights are shared among a large number of sub-architectures that have edges in common, with each considered a path within the supernet. The essential idea is to train one supernetwork that spans many options for the final design rather than generating and training thousands of networks independently.\n      </p>\n      <p>\n        NAS has traditionally required substantial computational resources due to its intensive training and evaluation phases, leading to a significant carbon footprint. To address this, <strong>NAS benchmarks</strong> have been developed, allowing rapid querying or prediction of final performance for neural architectures, drastically reducing computational requirements.\n      </p>\n      <p>\n        A <strong>NAS benchmark</strong> is a dataset structured with a predetermined train-test split, a defined search space, and a consistent training pipeline (with fixed hyperparameters). These benchmarks fall into two main categories:\n      </p>\n      <ol>\n        <li>\n          <strong>Surrogate NAS Benchmarks:</strong> These use a <strong>surrogate model</strong> (often a neural network) to predict the performance of an architecture within the search space, enabling faster evaluation by bypassing full training.\n        </li>\n        <li>\n          <strong>Tabular NAS Benchmarks:</strong> These provide a precomputed <strong>lookup table</strong> with actual performance metrics for architectures trained to convergence. Querying these tables yields precise results without needing to perform new, costly training runs.\n        </li>\n      </ol>\n      <p>\n        Both types of benchmarks are highly <strong>queryable</strong> and can simulate many NAS algorithms effectively, often using only a CPU for queries. This approach streamlines the NAS process, saving both time and energy by eliminating the need to train each candidate architecture from scratch.\n      </p>\n    </div>\n  );\n}\n\nexport default VIP;\n"],"mappings":";AAAA,OAAOA,KAAK,MAAM,OAAO;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAE1B,SAASC,GAAGA,CAAA,EAAG;EACb,oBACED,OAAA;IAAAE,QAAA,gBACEF,OAAA;MAAAE,QAAA,EAAI;IAAW;MAAAC,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAI,CAAC,eACpBN,OAAA;MAAAE,QAAA,gBACEF,OAAA;QAAAE,QAAA,EAAQ;MAAgC;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAQ,CAAC,yOAAqO,eAAAN,OAAA;QAAAE,QAAA,EAAQ;MAAyD;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAQ,CAAC,KAClW;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAG,CAAC,eACJN,OAAA;MAAAE,QAAA,GAAG,iCAC8B,eAAAF,OAAA;QAAAE,QAAA,EAAQ;MAAuB;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAQ,CAAC,qEACvE,eAAAN,OAAA;QAAAE,QAAA,gBACEF,OAAA;UAAAE,QAAA,EAAI;QAAgF;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAI,CAAC,eACzFN,OAAA;UAAAE,QAAA,EAAI;QAAuG;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAI,CAAC,eAChHN,OAAA;UAAAE,QAAA,EAAI;QAA+C;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAI,CAAC,eACxDN,OAAA;UAAAE,QAAA,EAAI;QAAqF;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAI,CAAC;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAC5F,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACJ,CAAC,eACJN,OAAA;MAAAE,QAAA,gBACEF,OAAA;QAAAE,QAAA,EAAQ;MAA0B;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAQ,CAAC,4SAC7C;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAG,CAAC,eACJN,OAAA;MAAAE,QAAA,GAAG,2MACwM,eAAAF,OAAA;QAAAE,QAAA,EAAQ;MAAmB;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAQ,CAAC,oCAAgC,EAAC,GAAG,eACjRN,OAAA;QAAGO,IAAI,EAAC,sDAAsD;QAACC,MAAM,EAAC,QAAQ;QAACC,GAAG,EAAC,qBAAqB;QAAAP,QAAA,EAAC;MAEzG;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAG,CAAC,EAAC,GAAG,EAAC,gYAEX;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAG,CAAC,eACJN,OAAA;MAAAE,QAAA,GAAG,sLACmL,eAAAF,OAAA;QAAAE,QAAA,EAAQ;MAAc;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAQ,CAAC,+JACrN;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAG,CAAC,eACJN,OAAA;MAAAE,QAAA,GAAG,IACC,eAAAF,OAAA;QAAAE,QAAA,EAAQ;MAAa;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAQ,CAAC,4MAClC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAG,CAAC,eACJN,OAAA;MAAAE,QAAA,gBACEF,OAAA;QAAAE,QAAA,gBACEF,OAAA;UAAAE,QAAA,EAAQ;QAAyB;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAQ,CAAC,iBAAa,eAAAN,OAAA;UAAAE,QAAA,EAAQ;QAAe;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAQ,CAAC,2JACzF;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC,eACLN,OAAA;QAAAE,QAAA,gBACEF,OAAA;UAAAE,QAAA,EAAQ;QAAuB;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAQ,CAAC,iCAA6B,eAAAN,OAAA;UAAAE,QAAA,EAAQ;QAAY;UAAAC,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAQ,CAAC,iLACpG;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACH,CAAC,eACLN,OAAA;MAAAE,QAAA,GAAG,sCACmC,eAAAF,OAAA;QAAAE,QAAA,EAAQ;MAAS;QAAAC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAQ,CAAC,4OAChE;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAG,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACD,CAAC;AAEV;AAACI,EAAA,GA7CQT,GAAG;AA+CZ,eAAeA,GAAG;AAAC,IAAAS,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}